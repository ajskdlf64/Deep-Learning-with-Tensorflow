{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 1.0\n",
    "\n",
    "텐서플로는 다양한 작업에대해 데이터 흐름 프로그래밍을 위한 오픈소스 소프트웨어 라이브러리이다. 심볼릭 수학 라이브러리이자, 뉴럴 네트워크같은 기계학습 응용프로그램에도 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple Linear Regression 를 TensorFlow 로 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(x) = W(x) + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3,4,5]\n",
    "y_data = [1,2,3,4,5]\n",
    "\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "# Hypothesis : y = Wx + b\n",
    "hypothesis = W * x_data + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_data, y_data, 'o')\n",
    "plt.ylim(0, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cost(W,b) = {1 \\over m} \\sum_{i=1}^{m}(H(x^{(i)}) - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.reduce_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=31, shape=(), dtype=float32, numpy=2.5>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = [1., 2., 3., 4.]\n",
    "tf.reduce_mean(v)   # 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.square()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=34, shape=(), dtype=int32, numpy=9>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(3)   # 9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$minimize\\;cost(W,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "A.assign_sub(B)\n",
    "A = A - B\n",
    "A -= B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = 2.4520, b = 0.3760\n"
     ]
    }
   ],
   "source": [
    "# learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Gradient Descent\n",
    "with tf.GradientTape() as tape : \n",
    "    hypothesis = W * x_data + b\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "W_grad, b_grad = tape.gradient(cost, [W,b])\n",
    "\n",
    "W.assign_sub(learning_rate * W_grad)\n",
    "b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "print('W = {:.4f}, b = {:.4f}' .format(W.numpy(),b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Parameter Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  i  |    W     |    b     |   cost   \n",
      "  0  |  2.4520  |  0.3760  |45.660004 \n",
      " 10  |  1.1036  |  0.0034  | 0.206336 \n",
      " 20  |  1.0128  | -0.0209  | 0.001026 \n",
      " 30  |  1.0065  | -0.0218  | 0.000093 \n",
      " 40  |  1.0059  | -0.0212  | 0.000083 \n",
      " 50  |  1.0057  | -0.0205  | 0.000077 \n",
      " 60  |  1.0055  | -0.0198  | 0.000072 \n",
      " 70  |  1.0053  | -0.0192  | 0.000067 \n",
      " 80  |  1.0051  | -0.0185  | 0.000063 \n",
      " 90  |  1.0050  | -0.0179  | 0.000059 \n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "print('{:^5}|{:^10}|{:^10}|{:^10}' .format('i', 'W', 'b', 'cost'))\n",
    "\n",
    "for i in range(100) :\n",
    "    \n",
    "    with tf.GradientTape() as tape : \n",
    "        hypothesis = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "        \n",
    "    W_grad, b_grad = tape.gradient(cost, [W,b])\n",
    "    \n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 10 == 0 : \n",
    "        print('{:^5}|{:^10.4f}|{:^10.4f}|{:^10.6f}' .format(i, W.numpy(), b.numpy(), cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(x) = Wx + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.0066934, shape=(), dtype=float32)\n",
      "tf.Tensor(2.9970603, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(W * 5.0 + b)\n",
    "print(W * 3.0 + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression and How to minimize cost 를 TensorFlow 로 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost function in pure python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cost(W) = {1 \\over m} \\sum_{i=1}^{m}(W(x_{i}) - y_{i})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([1,2,3])\n",
    "Y = np.array([1,2,3])\n",
    "\n",
    "def cost_fnc(W,X,Y) : \n",
    "    c = 0\n",
    "    for i in range(len(X)) : \n",
    "        c += (W * X[i] - Y[i]) ** 2\n",
    "    return c / len(X)\n",
    "\n",
    "for feed_W in np.linspace(-3, 5, num=15) :\n",
    "    curr_cost = cost_fnc(feed_W, X, Y)\n",
    "    print(\"{:6.3f} | {:10.5f}\" .format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost function in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([1,2,3])\n",
    "Y = np.array([1,2,3])\n",
    "\n",
    "def cost_fnc(W, X, Y) : \n",
    "    hypothesis = X * W\n",
    "    return tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "W_values = np.linspace(-3, 5, num=15)\n",
    "cost_values = []\n",
    "\n",
    "for feed_W in W_values :\n",
    "    curr_cost = cost_fnc(feed_W, X, Y)\n",
    "    cost_values.append(curr_cost)\n",
    "    print(\"{:6.3f} | {:10.5f}\" .format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cost(W) = {1 \\over m} \\sum_{i=1}^{m}(W(x_{i}) - y_{i})^2$\n",
    "\n",
    "==> $W := W - \\alpha {1 \\over m} \\sum_{i=1}^{m}(W(x_{i}) - y_{i})x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.00022470951>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "gradient = tf.reduce_mean(tf.multiply(tf.multiply(W,X) - Y, X))\n",
    "descent = W - tf.multiply(alpha, gradient)\n",
    "W.assign_sub(descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0| 1397.7949| -0.807653\n",
      "   10|    5.1347| -0.048951\n",
      "   20|    5.1347| -0.048951\n",
      "   30|    5.1347| -0.048951\n",
      "   40|    5.1347| -0.048951\n",
      "   50|    5.1347| -0.048951\n",
      "   60|    5.1347| -0.048951\n",
      "   70|    5.1347| -0.048951\n",
      "   80|    5.1347| -0.048951\n",
      "   90|    5.1347| -0.048951\n",
      "  100|    5.1347| -0.048951\n",
      "  110|    5.1347| -0.048951\n",
      "  120|    5.1347| -0.048951\n",
      "  130|    5.1347| -0.048951\n",
      "  140|    5.1347| -0.048951\n",
      "  150|    5.1347| -0.048951\n",
      "  160|    5.1347| -0.048951\n",
      "  170|    5.1347| -0.048951\n",
      "  180|    5.1347| -0.048951\n",
      "  190|    5.1347| -0.048951\n",
      "  200|    5.1347| -0.048951\n",
      "  210|    5.1347| -0.048951\n",
      "  220|    5.1347| -0.048951\n",
      "  230|    5.1347| -0.048951\n",
      "  240|    5.1347| -0.048951\n",
      "  250|    5.1347| -0.048951\n",
      "  260|    5.1347| -0.048951\n",
      "  270|    5.1347| -0.048951\n",
      "  280|    5.1347| -0.048951\n",
      "  290|    5.1347| -0.048951\n"
     ]
    }
   ],
   "source": [
    "# seed\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "# data\n",
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [1., 3., 5., 7.]\n",
    "\n",
    "# weight\n",
    "W = tf.Variable(tf.random.normal([1], -100, 100))\n",
    "\n",
    "# Gradient Descent\n",
    "for step in range(300) : \n",
    "    hypothesis = W * X\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    \n",
    "    alpha = 0.01\n",
    "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W,X) - Y, X))\n",
    "    descent = W - tf.multiply(alpha, gradient)\n",
    "    W.assign_sub(descent)\n",
    "    \n",
    "    if step % 10 == 0 : \n",
    "        print('{:5}|{:10.4f}|{:10.6f}' .format(step,cost.numpy(),W.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-variable Linear Regression 를 TensorFlow 로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |     551.4631\n",
      "   50 |       9.5045\n",
      "  100 |       3.4850\n",
      "  150 |       3.4123\n",
      "  200 |       3.4056\n",
      "  250 |       3.3996\n",
      "  300 |       3.3936\n",
      "  350 |       3.3877\n",
      "  400 |       3.3818\n",
      "  450 |       3.3759\n",
      "  500 |       3.3700\n",
      "  550 |       3.3641\n",
      "  600 |       3.3582\n",
      "  650 |       3.3523\n",
      "  700 |       3.3465\n",
      "  750 |       3.3406\n",
      "  800 |       3.3347\n",
      "  850 |       3.3290\n",
      "  900 |       3.3232\n",
      "  950 |       3.3175\n",
      " 1000 |       3.3117\n",
      " 1050 |       3.3060\n",
      " 1100 |       3.3003\n",
      " 1150 |       3.2945\n",
      " 1200 |       3.2889\n",
      " 1250 |       3.2831\n",
      " 1300 |       3.2774\n",
      " 1350 |       3.2718\n",
      " 1400 |       3.2661\n",
      " 1450 |       3.2605\n",
      " 1500 |       3.2549\n",
      " 1550 |       3.2493\n",
      " 1600 |       3.2438\n",
      " 1650 |       3.2382\n",
      " 1700 |       3.2326\n",
      " 1750 |       3.2271\n",
      " 1800 |       3.2215\n",
      " 1850 |       3.2161\n",
      " 1900 |       3.2106\n",
      " 1950 |       3.2051\n",
      " 2000 |       3.1996\n"
     ]
    }
   ],
   "source": [
    "# data and label\n",
    "x1 = [73., 93., 89., 96., 73]\n",
    "x2 = [80., 88., 91., 98., 66]\n",
    "x3 = [75., 93., 90., 100., 70]\n",
    "Y = [152., 185., 180., 196., 142]\n",
    "\n",
    "# random weights\n",
    "w1 = tf.Variable(tf.random_normal([1]))\n",
    "w2 = tf.Variable(tf.random_normal([1]))\n",
    "w3 = tf.Variable(tf.random_normal([1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.000001\n",
    "\n",
    "# Predict funciton\n",
    "def predict(X) :\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "n_epochs = 2000\n",
    "\n",
    "# Multi-Variable-Linear-Regression\n",
    "for i in range(n_epochs + 1) : \n",
    "    \n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape : \n",
    "        hypothesis = w1*x1 + w2*x2 + w3*x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "        \n",
    "    # calculates the gradients of the cost\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1,w2,w3,b])\n",
    "    \n",
    "    # update w1,w2,w3 and b\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    # process print\n",
    "    if i % 50 == 0 : \n",
    "        print(\"{:5} | {:12.4f}\" .format(i,cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(X) = XW$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X \n",
      " [[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]] \n",
      "\n",
      "y \n",
      " [[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]] \n",
      "\n",
      "W \n",
      " [[-0.3198804 ]\n",
      " [ 1.3575249 ]\n",
      " [-0.15776916]] \n",
      "\n",
      "b \n",
      " [0.80348307] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[73.,  80.,  75.,  152.],\n",
    "                 [93.,  88.,  93.,  185.],\n",
    "                 [89.,  91.,  90.,  180.],\n",
    "                 [96.,  98., 100.,  196.],\n",
    "                 [73.,  66.,  70.,  142.]], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "X = data[:, :-1]\n",
    "print('X','\\n',X,'\\n')\n",
    "y = data[:, [-1]]\n",
    "print('y','\\n',y,'\\n')\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,1]))\n",
    "print('W','\\n',W.numpy(),'\\n')\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "print('b','\\n',b.numpy(),'\\n')\n",
    "\n",
    "def predict(X) :\n",
    "    return tf.matmul(X,W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Logistic Regression/Classification 를 TensorFlow 로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c971fe3d601f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "x_train = [[1., 2.],\n",
    "           [2., 3.],\n",
    "           [3., 1.],\n",
    "           [4., 3.],\n",
    "           [5., 3.],\n",
    "           [6., 2.]]\n",
    "\n",
    "y_train = [[0.],\n",
    "           [0.],\n",
    "           [0.],\n",
    "           [1.],\n",
    "           [1.],\n",
    "           [1.]]\n",
    "\n",
    "x_test = [[5., 2.]]\n",
    "y_test = [[1.]]\n",
    "\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
    "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')\n",
    "\n",
    "def logistic_regression(features) : \n",
    "    hypothesis = tf.div(1., 1. + tf.exp(tf.matmul(feature, W) + b))\n",
    "    return hypothesis\n",
    "\n",
    "def loss_fn(features, labels) : \n",
    "    hypothesis = logistic_regression(features)\n",
    "    cost = -tf.reduce_mean(labels * tf.log(loss_fn(hypothesis) + (1 - labels) * tf.log(1 - hypothesis)))\n",
    "    return cost\n",
    "\n",
    "def grad(hypothesis, features, labels) : \n",
    "    with tf.GradientTape() as tape : \n",
    "        loss_value = loss_fn(hypothesis, labels)\n",
    "    return tape.gradient(loss_value, [W,b])\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "\n",
    "EPOCHS = 2000\n",
    "\n",
    "for step in range(EPOCHS) : \n",
    "    for feature, labels in tfe.Iterator(dataset) : \n",
    "        grads = grad(logistic_regression(features), features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, [W,b]))\n",
    "        if step % 100 == 0 : \n",
    "            print('iter : {}, Loss : {:.4f}' .format(step, loss_fn(logistic_regression(features), labels)))\n",
    "            \n",
    "def accuracy_fn(hypothesis, labels) : \n",
    "    predicted = tf.cast(hypothesis, labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
    "    return accuracy\n",
    "\n",
    "test_acc = accuracy_fn(logistic_regression(x_test), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
